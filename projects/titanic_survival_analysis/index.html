<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Titanic Survival Prediction | Geoffrey Karnbach </title> <meta name="author" content="Geoffrey Karnbach"> <meta name="description" content="Titanic Survival Prediction - Will John Doe survive the titanic? - Probably not!"> <meta name="keywords" content="geoffrey karnbach, dergoyote, geoffrey karnbach portfolio, LÄNDR, Recipe Planer"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%98%8E&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://geoffreykarnbach.github.io/projects/titanic_survival_analysis/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Geoffrey</span> Karnbach </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/github/">GitHub </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Titanic Survival Prediction</h1> <p class="post-description">Titanic Survival Prediction - Will John Doe survive the titanic? - Probably not!</p> </header> <article> <h4 id="source-code--live-demo">Source code + Live demo</h4> <p><a href="https://github.com/GeoffreyKarnbach/Titanic-Survival-Analysis" rel="external nofollow noopener" target="_blank">Go to the GitHub repository!</a></p> <p>Check it out live: <a href="https://titanic-survival-analysis.vercel.app/" rel="external nofollow noopener" target="_blank">https://titanic-survival-analysis.vercel.app/</a></p> <h4 id="concept">Concept</h4> <p>In this project, an extensive analysis of the Titanic dataset from Kaggle was conducted to develop and fine-tune various machine learning models aimed at predicting passenger survival outcomes. The following outlines the data processing steps and techniques employed to enhance model accuracy and reliability.</p> <h4 id="index">Index</h4> <ol> <li><a href="#data-preparation-and-cleaning">Data Preparation and Cleaning</a></li> <li><a href="#dataset">Dataset</a></li> <li><a href="#model-development-and-evaluation">Model Development and Evaluation</a></li> <li><a href="#benchmarks---model-accuracy">Benchmarks - Model Accuracy</a></li> <li> <a href="#decision-tree">Decision Tree</a> <ul> <li> <a href="#decision-tree-models-overview">Decision Tree Models Overview</a> <ul> <li><a href="#decision-tree-model-v1">Decision Tree Model v1</a></li> <li><a href="#decision-tree-model-v2">Decision Tree Model v2</a></li> <li><a href="#decision-tree-model-v3">Decision Tree Model v3</a></li> <li><a href="#decision-tree-model-v4">Decision Tree Model v4</a></li> <li><a href="#decision-tree-model-v5">Decision Tree Model v5</a></li> <li><a href="#decision-tree-model-v6">Decision Tree Model v6</a></li> <li><a href="#decision-tree-model-v7">Decision Tree Model v7</a></li> </ul> </li> </ul> </li> <li> <a href="#support-vector-machines-svm">Support Vector Machines (SVM)</a> <ul> <li> <a href="#svm-models-overview">SVM Models Overview</a> <ul> <li><a href="#svm-default-model">SVM Default Model</a></li> <li><a href="#svm-linear-kernel">SVM Linear Kernel</a></li> <li><a href="#svm-polynomial-kernel">SVM Polynomial Kernel</a></li> <li><a href="#svm-rbf-kernel">SVM RBF Kernel</a></li> <li><a href="#svm-sigmoid-kernel">SVM Sigmoid Kernel</a></li> </ul> </li> </ul> </li> <li> <a href="#ensemble-methods">Ensemble Methods</a> <ul> <li><a href="#random-forest">Random Forest</a></li> <li><a href="#gradient-boosting">Gradient Boosting</a></li> </ul> </li> <li><a href="#k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</a></li> <li><a href="#logistic-regression">Logistic Regression</a></li> <li><a href="#images">Images</a></li> <li><a href="#getting-started">Getting Started</a></li> <li><a href="#contact">Contact</a></li> <li><a href="#acknowledgements">Acknowledgements</a></li> </ol> <h4 id="data-preparation-and-cleaning"><u>Data Preparation and Cleaning</u></h4> <p>The initial phase involved cleaning and preparing the dataset through:</p> <ul> <li> <strong>Attribute Selection</strong>: Excluded non-relevant attributes such as PassengerId, Ticket Number, and Cabin Number, focusing on Passenger Class, Name, Sex, Age, Number of siblings/spouses, Number of parents/children, Fare, and Embarkation Port.</li> <li> <p><strong>Handling Missing Data</strong>: Used several imputation techniques to address missing values:</p> <ul> <li> <strong>Mode Imputation</strong>: For categorical variables, filling missing values with the most frequently occurring value.</li> <li> <strong>Median Imputation</strong>: Replacing missing numeric values with the median value.</li> <li> <strong>K-Nearest Neighbors (KNN) Imputation</strong>: Estimating missing values based on the values of the nearest neighbors.</li> <li> <strong>Multivariate Imputation by Chained Equations (MICE)</strong>: Advanced imputation that estimates missing values based on predictions from other variables.</li> </ul> </li> <li> <strong>Feature Engineering</strong>: <ul> <li> <strong>Categorization of Continuous Variables</strong>: Divided Age and Fare into meaningful categories.</li> <li> <strong>New Feature Creation</strong>: Added features such as <code class="language-plaintext highlighter-rouge">isAlone</code> (indicating whether a passenger was traveling alone) and <code class="language-plaintext highlighter-rouge">title</code> (extracted from passenger names).</li> </ul> </li> </ul> <h4 id="dataset"><u>Dataset</u></h4> <p>The processed dataset, available for <a href="https://dergoyote.eu.pythonanywhere.com/dataset" rel="external nofollow noopener" target="_blank">download</a>, contains the following attributes:</p> <ul> <li> <strong>Pclass</strong>: Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)</li> <li> <strong>Sex</strong>: Passenger Gender (0 = Male, 1 = Female)</li> <li> <strong>Age</strong>: Passenger Age (0 = 0–13, 1 = 13–18, 2 = 18–60, 3 = 60–100)</li> <li> <strong>Fare</strong>: Ticket Fare (0 = 0–50, 1 = 50–100, 2 = 100–200, 3 = 200–1000)</li> <li> <strong>Embarked</strong>: Port of Embarkation (0 = Southampton, 1 = Cherbourg, 2 = Queenstown)</li> <li> <strong>Relatives</strong>: Number of Relatives (Sibling, Spouse, Parent, Child)</li> <li> <strong>IsAlone</strong>: Traveling Alone (0 = No, 1 = Yes)</li> <li> <strong>Title</strong>: Passenger Title (0 = Mr, 1 = Mrs, 2 = Miss, 3 = Master, 4 = Others)</li> </ul> <p>The target variable is <code class="language-plaintext highlighter-rouge">Survived</code>, indicating whether the passenger survived (1) or perished (0).</p> <h4 id="model-development-and-evaluation"><u>Model Development and Evaluation</u></h4> <p>Multiple machine learning models were developed and evaluated on the processed dataset. The following models were implemented:</p> <ul> <li>Decision Tree Classifier</li> <li>Support Vector Machines (SVM)</li> <li>Random Forest Classifier / Gradient Boosting Classifier</li> <li>K-Nearest Neighbors (KNN)</li> <li>Logistic Regression</li> </ul> <h4 id="benchmarks---model-accuracy"><u>Benchmarks - Model Accuracy</u></h4> <p>Each model’s performance was compared against a perfect prediction benchmark (100% accuracy):</p> <table> <thead> <tr> <th>Model</th> <th>Accuracy</th> </tr> </thead> <tbody> <tr> <td>Decision Tree Classifier v1</td> <td>60.53%</td> </tr> <tr> <td>Decision Tree Classifier v2</td> <td>70.33%</td> </tr> <tr> <td>Decision Tree Classifier v3</td> <td>66.99%</td> </tr> <tr> <td>Decision Tree Classifier v4</td> <td>67.46%</td> </tr> <tr> <td>Decision Tree Classifier v5</td> <td>70.33%</td> </tr> <tr> <td>Decision Tree Classifier v6</td> <td>70.33%</td> </tr> <tr> <td>Decision Tree Classifier v7</td> <td>72.25%</td> </tr> <tr> <td>Gradient Boosting</td> <td>75.60%</td> </tr> <tr> <td>K-Nearest Neighbors (KNN)</td> <td>75.60%</td> </tr> <tr> <td>Logistic Regression</td> <td>75.84%</td> </tr> <tr> <td>Random Forest</td> <td>72.73%</td> </tr> <tr> <td>SVM Default</td> <td>77.03%</td> </tr> <tr> <td>SVM Linear Kernel</td> <td>75.60%</td> </tr> <tr> <td>SVM Polynomial Kernel</td> <td>74.88%</td> </tr> <tr> <td>SVM RBF Kernel</td> <td>79.90%</td> </tr> <tr> <td>SVM Sigmoid Kernel</td> <td>64.35%</td> </tr> </tbody> </table> <p><br></p> <h4 id="decision-tree"><u>Decision Tree</u></h4> <p>Decision Trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by splitting the data into subsets based on the value of input features. This process is repeated recursively, creating a tree-like model of decisions and their possible consequences.</p> <p>Each node in the tree represents a feature, each branch represents a decision rule, and each leaf node represents an outcome (label). The goal is to create a model that predicts the target variable by learning simple decision rules inferred from the data features.</p> <p>Key advantages of decision trees include their simplicity, interpretability, and ability to handle both numerical and categorical data. However, they can be prone to overfitting, especially with noisy data.</p> <p>The decision tree models presented here were implemented using the <strong>scikit-learn</strong> library in Python. Scikit-learn provides a robust and easy-to-use implementation of decision trees and other machine learning algorithms, making it a popular choice for data scientists and researchers.</p> <p>In scikit-learn, the <code class="language-plaintext highlighter-rouge">DecisionTreeClassifier</code> class is used for creating decision tree models. We only used the <code class="language-plaintext highlighter-rouge">random_state</code> hyperparameter to ensure reproducibility of the results.</p> <p>With the prepared dataset, several decision tree models were developed and evaluated. Each iteration refined the approach, resulting in the following improvements:</p> <ul> <li> <strong>Decision Tree Version 1:</strong> Achieved an initial accuracy of 60.53%.</li> <li> <strong>Decision Tree Version 2:</strong> Improved accuracy to 70.33% by categorizing continuous variables.</li> <li> <strong>Decision Tree Versions 3 to 6:</strong> Implemented advanced imputation techniques, maintaining accuracy around 70.33%.</li> <li> <strong>Decision Tree Version 7:</strong> Introduced feature engineering, resulting in a final accuracy of 72.25%.</li> </ul> <p>This structured approach to data processing and feature engineering significantly enhanced the predictive power of the decision tree models, offering valuable insights into the factors influencing Titanic survival rates.</p> <h4 id="decision-tree-model-v1">Decision Tree Model v1</h4> <p>In Decision Tree Model v1, we focused on using the most relevant attributes identified from the dataset. The chosen attributes were:</p> <ul> <li>Sex</li> <li>Passenger Class</li> <li>Number of co-travelers</li> <li>Age</li> <li>Fare</li> <li>Embarkation port</li> </ul> <p>Model performance was initially evaluated, with an accuracy of about 60.53%. This version served as a foundation for further enhancements.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v1_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v1_graphviz.png" alt="Decision Tree Model v1" width="1000"> </a></p> <h4 id="decision-tree-model-v2">Decision Tree Model v2</h4> <p>For Decision Tree Model v2, we made significant improvements by categorizing attributes more effectively:</p> <ul> <li>Properly categorizing attributes in our training set (e.g., sex and embarked as integers).</li> <li>Dividing fare and age into different categories (e.g., fare &lt; 50, 50 - 100, 100 - 200, &gt; 200).</li> </ul> <p>This version aimed to refine the model’s accuracy by addressing previous shortcomings. By splitting the age and fare into distinct categories, we observed an increase to 70.33% accuracy (+9.8%).</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v2_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v2_graphviz.png" alt="Decision Tree Model v2" width="1000"> </a></p> <h4 id="decision-tree-model-v3">Decision Tree Model v3</h4> <p>Decision Tree Model v3 implemented the first imputation method:</p> <ul> <li> <strong>Mode Imputation:</strong> Missing values are replaced with the most frequently occurring value (mode) in the column.</li> </ul> <p>Despite this method, the accuracy achieved was around 66.99%, which was slightly lower than version 2 but similar to the subsequent versions.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v3_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v3_graphviz.png" alt="Decision Tree Model v3" width="1000"> </a></p> <h4 id="decision-tree-model-v4">Decision Tree Model v4</h4> <p>In Decision Tree Model v4, we utilized the second imputation method:</p> <ul> <li> <strong>Median Imputation:</strong> Missing values are replaced with the median value of the column, which is less sensitive to outliers compared to the mode.</li> </ul> <p>Accuracy for this version was around 67.46%, indicating marginal improvement over version 3 but still not surpassing version 2.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v4_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v4_graphviz.png" alt="Decision Tree Model v4" width="1000"> </a></p> <h4 id="decision-tree-model-v5">Decision Tree Model v5</h4> <p>Decision Tree Model v5 applied the third imputation method:</p> <ul> <li> <strong>KNN (K-Nearest Neighbors) Imputation:</strong> Missing values are estimated based on the values of the nearest neighbors in the feature space.</li> </ul> <p>This method achieved an accuracy of 70.33%, matching the results from version 2.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v5_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v5_graphviz.png" alt="Decision Tree Model v5" width="1000"> </a></p> <h4 id="decision-tree-model-v6">Decision Tree Model v6</h4> <p>For Decision Tree Model v6, we employed the fourth imputation method:</p> <ul> <li> <strong>MICE (Multivariate Imputation by Chained Equations):</strong> This method performs multiple imputations based on a series of regressions on other features to estimate missing values.</li> </ul> <p>The accuracy achieved with MICE was consistent at 70.33%, similar to version 5 and version 2.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v6_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v6_graphviz.png" alt="Decision Tree Model v6" width="1000"> </a></p> <h4 id="decision-tree-model-v7">Decision Tree Model v7</h4> <p>In Decision Tree Model v7, we introduced feature engineering with two new features:</p> <ul> <li>isAlone: Indicates whether the passenger is traveling alone.</li> <li>title: A title extracted from the passenger’s name.</li> </ul> <p>These new features contributed to a noticeable increase in accuracy, reaching approximately 72.25%.</p> <p><a href="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v7_graphviz.png" target="_blank"> <img src="../../assets/img/titanic_survival_prediction/titanic_decision_tree_model_v7_graphviz.png" alt="Decision Tree Model v7" width="1000"> </a></p> <h4 id="support-vector-machines-svm"><u>Support Vector Machines (SVM)</u></h4> <p>Support Vector Machines (SVM) are supervised learning models used for classification and regression analysis. SVMs are effective in high-dimensional spaces and are versatile due to the use of different kernel functions.</p> <p>An SVM constructs a hyperplane or set of hyperplanes in a high-dimensional space, which can be used for classification, regression, or other tasks. The goal is to find a hyperplane that best separates the classes in the training dataset.</p> <p>Several SVM models were developed and evaluated using the Titanic dataset with the following features: Pclass, Sex, Age, Fare, Embarked, Relatives, IsAlone, and Title. Each model used a different kernel function to map the input features into higher-dimensional spaces, resulting in the following performances:</p> <ul> <li> <strong>SVM with Linear Kernel:</strong> Utilizes a linear hyperplane for classification.</li> <li> <strong>SVM with Polynomial Kernel:</strong> Maps features into a higher-dimensional space using polynomial functions.</li> <li> <strong>SVM with RBF Kernel:</strong> Uses a Radial Basis Function (RBF) to create a more flexible decision boundary.</li> <li> <strong>SVM with Sigmoid Kernel:</strong> Employs a sigmoid function to map the features, similar to a neural network.</li> </ul> <p>This structured approach to model building allowed for a comparison of different SVM configurations, highlighting the strengths and weaknesses of each kernel function.</p> <h4 id="svm-default-model">SVM Default Model</h4> <p>The default SVM model was built using the basic SVC class from scikit-learn without specifying any kernel. The chosen attributes were:</p> <ul> <li>Pclass</li> <li>Sex</li> <li>Age</li> <li>Fare</li> <li>Embarked</li> <li>Relatives</li> <li>IsAlone</li> <li>Title</li> </ul> <p>Model performance was evaluated using the entire dataset, serving as a baseline for further improvements with kernel-specific models. The accuracy achieved was approximately 77.03%.</p> <h4 id="svm-linear-kernel">SVM Linear Kernel</h4> <p>The SVM model with a linear kernel was built to classify the data using a linear decision boundary. The hyperparameters used were:</p> <ul> <li> <strong>Kernel: linear</strong> - This specifies that the linear kernel should be used.</li> <li> <strong>C: 1</strong> - The regularization parameter, which controls the trade-off between achieving a low training error and a low testing error. A lower C value makes the decision surface smoother, while a higher C aims to classify all training examples correctly.</li> </ul> <p>This model provided a straightforward approach to classification, leveraging the linear relationships in the data. The accuracy achieved was approximately 75.60%.</p> <h4 id="svm-polynomial-kernel">SVM Polynomial Kernel</h4> <p>The SVM model with a polynomial kernel transformed the features into higher-dimensional space using polynomial functions. The hyperparameters used were:</p> <ul> <li> <strong>Kernel: poly</strong> - This specifies that the polynomial kernel should be used.</li> <li> <strong>Degree: 3</strong> - The degree of the polynomial function, which determines the flexibility of the decision boundary.</li> <li> <strong>coef0: 0</strong> - This is a free parameter that adjusts the influence of higher-order terms in the polynomial.</li> </ul> <p>This approach allowed for capturing more complex relationships in the data, providing more flexibility in the decision boundary. The accuracy achieved was approximately 74.88%.</p> <h4 id="svm-rbf-kernel">SVM RBF Kernel</h4> <p>The SVM model with a Radial Basis Function (RBF) kernel created a flexible decision boundary by mapping the features into an infinite-dimensional space. The hyperparameters used were:</p> <ul> <li> <strong>Kernel: rbf</strong> - This specifies that the RBF kernel should be used.</li> <li> <strong>C: 0.5</strong> - The regularization parameter, which controls the trade-off between achieving a low training error and a low testing error. A lower C value makes the decision surface smoother.</li> <li> <strong>Gamma: 0.3</strong> - The gamma parameter defines how far the influence of a single training example reaches. A low gamma value means ‘far’ and a high gamma value means ‘close’. Gamma is inversely proportional to the standard deviation of the RBF kernel.</li> </ul> <p>This kernel is effective in situations where the relationship between features is not linear, allowing for more complex decision boundaries. The accuracy achieved was approximately 79.90%.</p> <h4 id="svm-sigmoid-kernel">SVM Sigmoid Kernel</h4> <p>The SVM model with a sigmoid kernel used a sigmoid function to map the features, akin to the activation function in neural networks. The hyperparameters used were:</p> <ul> <li> <strong>Kernel: sigmoid</strong> - This specifies that the sigmoid kernel should be used.</li> <li> <strong>Gamma: scale</strong> - The gamma parameter defines how far the influence of a single training example reaches. ‘Scale’ is a value that depends on the number of features, equivalent to 1 / (number of features * X.var()) as a default.</li> </ul> <p>This model captured non-linear relationships in the data, providing a decision boundary similar to neural network models. The accuracy achieved was approximately 64.35%.</p> <h4 id="ensemble-methods"><u>Ensemble Methods</u></h4> <p>Ensemble methods are advanced machine learning techniques that combine multiple models to improve overall performance. By aggregating the predictions from several base models, these methods aim to enhance accuracy, robustness, and generalizability.</p> <p>Two popular ensemble methods are Random Forest and Gradient Boosting. Both techniques leverage multiple decision trees but in different ways to create a stronger predictive model.</p> <h4 id="random-forest">Random Forest</h4> <p>Random Forest is an ensemble learning method that builds multiple decision trees during training. Each tree is constructed using a different subset of the training data and a random subset of features. The final prediction is made by aggregating the results of all the trees: for classification, this means taking the mode (most common class) of all the trees’ predictions, and for regression, it means taking the mean of the predictions.</p> <p>This approach has several advantages, including:</p> <ul> <li> <strong>Robustness to Overfitting:</strong> By averaging the predictions of multiple trees, Random Forest reduces the risk of overfitting compared to a single decision tree.</li> <li> <strong>Feature Importance:</strong> It provides insights into the importance of different features in making predictions, which can be useful for feature selection.</li> <li> <strong>Handling of Missing Data:</strong> Random Forest can handle missing values and maintain accuracy even when some features are missing.</li> </ul> <p>The Random Forest model for the Titanic dataset achieved an accuracy of <strong>72.73%</strong>. This result reflects the model’s effectiveness in classifying the Titanic survival data by using multiple decision trees.</p> <h4 id="gradient-boosting">Gradient Boosting</h4> <p>Gradient Boosting is an ensemble technique that builds models sequentially. Unlike Random Forest, which creates multiple trees independently, Gradient Boosting constructs each new tree to correct the errors made by the previous trees. This process involves fitting new models to the residual errors of the existing models, gradually improving the predictive performance.</p> <p>Key characteristics of Gradient Boosting include:</p> <ul> <li> <strong>Sequential Model Building:</strong> Each new model focuses on the errors of the previous models, which helps in correcting mistakes and improving accuracy.</li> <li> <strong>Flexibility:</strong> Gradient Boosting can work with different types of loss functions and base learners, making it adaptable to various problems.</li> <li> <strong>Overfitting Control:</strong> Techniques such as regularization and early stopping are used to prevent overfitting and enhance the model’s generalizability.</li> </ul> <p>The Gradient Boosting model for the Titanic dataset achieved an accuracy of <strong>75.60%</strong>. This higher accuracy demonstrates the model’s ability to improve predictive performance by correcting previous errors and fine-tuning its predictions.</p> <h4 id="k-nearest-neighbors-knn"><u>K-Nearest Neighbors (KNN)</u></h4> <p>K-Nearest Neighbors (KNN) is a simple yet effective machine learning algorithm used for both classification and regression tasks. The core idea behind KNN is to predict the class or value of a data point based on the classes or values of its nearest neighbors in the feature space.</p> <p>In KNN, the prediction for a new data point is determined by looking at the ‘k’ nearest points in the training dataset. The algorithm then assigns the most common class among these nearest neighbors (for classification) or the average of their values (for regression) to the new data point.</p> <h4 id="how-knn-works">How KNN Works</h4> <p>The KNN algorithm involves the following steps:</p> <ul> <li> <strong>Distance Calculation:</strong> Calculate the distance between the new data point and all points in the training dataset using a distance metric, typically Euclidean distance.</li> <li> <strong>Finding Neighbors:</strong> Identify the ‘k’ closest points to the new data point based on the calculated distances.</li> <li> <strong>Prediction:</strong> For classification, assign the most frequent class among the ‘k’ nearest neighbors. For regression, compute the average of the values of the ‘k’ nearest neighbors.</li> </ul> <p>KNN is known for its simplicity and ease of implementation. However, its performance can be sensitive to the choice of ‘k’ and the distance metric. It can also be computationally expensive with large datasets, as it requires distance calculations for every prediction.</p> <h4 id="performance-on-titanic-dataset">Performance on Titanic Dataset</h4> <p>The K-Nearest Neighbors model for the Titanic dataset was trained and evaluated with the hyperparameter <strong>n_neighbors set to 20</strong>. This tuning resulted in an accuracy of <strong>75.60%</strong>, a significant improvement from the initial accuracy. By optimizing the number of neighbors, the model was able to enhance its performance and achieve accuracy comparable to the Gradient Boosting model.</p> <p>Despite this improvement, KNN’s performance is on par with Gradient Boosting but slightly better than Random Forest, which achieved an accuracy of <strong>72.73%</strong>. While KNN’s accuracy is competitive, the model’s performance is sensitive to the choice of ‘k’ and can be computationally intensive for larger datasets.</p> <h4 id="logistic-regression"><u>Logistic Regression</u></h4> <p>Logistic Regression is a widely used statistical method for binary classification problems. It models the probability of a binary outcome based on one or more predictor variables. Despite its name, Logistic Regression is a classification algorithm, not a regression algorithm.</p> <p>The core idea of Logistic Regression is to use the logistic function (or sigmoid function) to model the probability of the positive class. This function outputs values between 0 and 1, which can be interpreted as probabilities. The model then classifies data points based on a threshold value (typically 0.5).</p> <h4 id="how-logistic-regression-works">How Logistic Regression Works</h4> <p>Logistic Regression follows these steps:</p> <ul> <li> <strong>Modeling Probabilities:</strong> The logistic function is used to transform the linear combination of input features into a probability value between 0 and 1.</li> <li> <strong>Thresholding:</strong> The predicted probability is compared to a threshold to classify the outcome. By default, this threshold is 0.5, but it can be adjusted based on specific needs.</li> <li> <strong>Training:</strong> The model is trained by finding the parameters (coefficients) that maximize the likelihood of the observed data. This is typically done using optimization algorithms like Gradient Descent.</li> </ul> <p>Logistic Regression is valued for its simplicity and interpretability. It provides insights into how features impact the probability of the outcome and is computationally efficient, making it suitable for large datasets.</p> <h4 id="performance-on-titanic-dataset-1">Performance on Titanic Dataset</h4> <p>The Logistic Regression model for the Titanic dataset was trained and evaluated, achieving an accuracy of <strong>75.84%</strong>. This performance reflects the model’s ability to effectively classify the Titanic survival data, demonstrating its robustness in handling binary classification tasks.</p> <p>Logistic Regression performed slightly better than K-Nearest Neighbors and Random Forest models, which achieved accuracies of <strong>75.60%</strong> and <strong>72.73%</strong>, respectively. While not the top performer, Logistic Regression offers a balance between interpretability and predictive performance.</p> <h4 id="images"><u>Images</u></h4> <p><br> <img src="../../assets/img/titanic_survival_prediction/1.png" alt="Start Page" width="1000"> <img src="../../assets/img/titanic_survival_prediction/2.png" alt="Decision Tree" width="1000"> <img src="../../assets/img/titanic_survival_prediction/3.png" alt="SVM" width="1000"> <img src="../../assets/img/titanic_survival_prediction/4.png" alt="Ensemble" width="1000"> <img src="../../assets/img/titanic_survival_prediction/5.png" alt="KNN" width="1000"> <img src="../../assets/img/titanic_survival_prediction/6.png" alt="Logistic Regression" width="1000"> <img src="../../assets/img/titanic_survival_prediction/7.png" alt="Prediction Form" width="1000"> <img src="../../assets/img/titanic_survival_prediction/8.png" alt="Prediction Result" width="1000"></p> <h4 id="getting-started"><u>Getting Started</u></h4> <p>To replicate or extend this project, follow these steps:</p> <ol> <li> <p><strong>Clone the Repository</strong>:</p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>git clone https://github.com/GeoffreyKarnbach/Titanic-Survival-Analysis
</code></pre></div> </div> </li> <li> <p><strong>Run the initial build command to setup the development environment</strong></p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>sh build_all.sh
</code></pre></div> </div> <p>This shell scripts installs all dependencies with pip, runs the notebook to generate the refined datasets for the machine learning models, builds the models, evaluates them on the test dataset and ranks them according to the solution file.</p> <p>Once the shell script has been run, the backend can be started (all models are ready to be accessed over the prediction API).</p> </li> <li> <p><strong>Run the frontend</strong></p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>Frontend
npm <span class="nb">install
</span>ng serve
</code></pre></div> </div> </li> <li> <p><strong>Run the prediction python backend service</strong></p> <div class="language-bash highlighter-rouge"> <div class="highlight"><pre class="highlight"><code>python titanic_service.py
</code></pre></div> </div> </li> <li> <p><strong>Visit the frontend page</strong></p> <p>Go to: <a href="http://localhost:4200/" rel="external nofollow noopener" target="_blank">http://localhost:4200/</a></p> </li> </ol> <h4 id="contact"><u>Contact</u></h4> <p>For any questions or suggestions, please contact:</p> <ul> <li>Author: Geoffrey Karnbach</li> <li>GitHub: <a href="https://github.com/GeoffreyKarnbach" rel="external nofollow noopener" target="_blank">https://github.com/GeoffreyKarnbach</a> </li> </ul> <h4 id="acknowledgements"><u>Acknowledgements</u></h4> <p>The Titanic dataset is provided by Kaggle and is used here for educational and analytical purposes.</p> <p><a href="https://www.kaggle.com/competitions/titanic" rel="external nofollow noopener" target="_blank">https://www.kaggle.com/competitions/titanic</a></p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Geoffrey Karnbach. Last updated: July 29, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-projects",title:"Projects",description:"A selection of some of my projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-github",title:"GitHub",description:"",section:"Navigation",handler:()=>{window.location.href="/github/"}},{id:"nav-cv",title:"CV",description:"TODO",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"news-lt-u-gt-titanic-survival-prediction-will-john-doe-survive-the-titanic-probably-not-lt-u-gt-lt-br-gt-check-out-lt-a-href-quot-https-geoffreykarnbach-github-io-projects-titanic-survival-analysis-quot-gt-the-project-page-lt-a-gt-lt-a-href-quot-https-github-com-geoffreykarnbach-titanic-survival-analysis-quot-gt-the-source-code-lt-a-gt-and-lt-a-href-quot-https-titanic-survival-analysis-vercel-app-quot-gt-the-live-demo-lt-a-gt",title:"&lt;u&gt;Titanic Survival Prediction - Will John Doe survive the titanic? - Probably not!&lt;/u&gt;&lt;br /&gt;Check out &lt;a href=&quot;https://geoffreykarnbach.github.io/projects/titanic_survival_analysis/&quot;&gt;the project page&lt;/a&gt;, &lt;a href=&quot;https://github.com/GeoffreyKarnbach/Titanic-Survival-Analysis&quot;&gt;the source code&lt;/a&gt; and &lt;a href=&quot;https://titanic-survival-analysis.vercel.app/&quot;&gt;the live demo&lt;/a&gt;!",description:"",section:"News"},{id:"projects-cryptopay",title:"CryptoPay",description:"CryptoPay, a simple initiative to allow paying with cryptocurrencies in a physical location, as simply as with a credit card.",section:"Projects",handler:()=>{window.location.href="/projects/cryptopay/"}},{id:"projects-fake-news-detector-ai",title:"Fake News Detector AI",description:"Detect if a specific article is a fake news or not.",section:"Projects",handler:()=>{window.location.href="/projects/fake_news_detector/"}},{id:"projects-gym-companion-app",title:"Gym Companion App",description:"Transform your workouts with the Gym Companion App \u2013 track, analyze, and optimize your progress seamlessly!",section:"Projects",handler:()=>{window.location.href="/projects/gym_companion_app/"}},{id:"projects-jeffos",title:"JeffOS",description:"JeffOS, an interactive and educational virtual CPU and OS environment!",section:"Projects",handler:()=>{window.location.href="/projects/jeff_os/"}},{id:"projects-l\xe4ndr",title:"L\xe4ndr",description:"Your go-to platform for easily finding and booking the best party locations.",section:"Projects",handler:()=>{window.location.href="/projects/laendr/"}},{id:"projects-python-datastructures",title:"Python Datastructures",description:"Collection of self-implemented datastructure with python, including a Queue, Stack, BST, Hashmap, LL, Heap...",section:"Projects",handler:()=>{window.location.href="/projects/python_datastructures/"}},{id:"projects-recipe-planer",title:"Recipe Planer",description:"Your all-in-one digital cookbook and smart grocery planner for seamless meal preparation and shopping!",section:"Projects",handler:()=>{window.location.href="/projects/recipe_planer/"}},{id:"projects-spotify-language-detection",title:"Spotify Language Detection",description:"Effortlessly organize your Spotify playlist by song language with our own language detection model.",section:"Projects",handler:()=>{window.location.href="/projects/spotify_language_detection/"}},{id:"projects-task-automation",title:"Task automation",description:"Effortlessly schedule and manage your Python scripts without the hassle of cron jobs!",section:"Projects",handler:()=>{window.location.href="/projects/task_automation/"}},{id:"projects-titanic-survival-prediction",title:"Titanic Survival Prediction",description:"Titanic Survival Prediction - Will John Doe survive the titanic? - Probably not!",section:"Projects",handler:()=>{window.location.href="/projects/titanic_survival_analysis/"}},{id:"projects-vienna-subway-simulation",title:"Vienna Subway Simulation",description:"Vienna Subway Simulation - Navigate Vienna&#39;s subway like a pro with our simulation, powered by the Dijkstra Algorithm for the shortest routes!",section:"Projects",handler:()=>{window.location.href="/projects/vienna_subway_simulation/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%67%65%6F%66%66%72%65%79.%6B%61%72%6E%62%61%63%68@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/GeoffreyKarnbach","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/geoffrey-karnbach-b849101b9","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>